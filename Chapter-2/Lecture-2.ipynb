{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "* Introduce programming concepts into your conversations with the LLM.\n",
    "* A Prompt Template is a predefined structure that helps generate inputs (prompts) for language models in a reusable, consistent, and flexible way. It serves as a blueprint for creating dynamic prompts by incorporating variables and specific instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "* Input: the prompt we send to the LLM.\n",
    "* Output: the response from the LLM.\n",
    "* We can switch LLMs and use several different LLMs. \n",
    "\n",
    "### Why Are Prompt Templates Necessary?\n",
    "- **Consistency:** Ensures that prompts follow a specific structure, reducing errors and ambiguities.\n",
    "- **Reusability:** Templates can be reused across multiple tasks or workflows.\n",
    "- **Dynamic Input Handling:** Supports variable substitution to customize prompts for different contexts.\n",
    "- **Improved Performance:** Provides structured and optimized inputs for language models, improving their effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorization of Prompt Types:\n",
    "\n",
    "- **Template Types** are categorized based on the type or role of the message (system, human, AI) and how the prompt is structured.\n",
    "- **Shot Types** are categorized based on how much context or how many examples (messages) you provide to help the model understand the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Template Types (Categorization Based on Type) \n",
    "* These are categories based on the structure or format of the prompt. They focus on what type of message or content is being used in the conversation. For example:\n",
    "\n",
    "##### 1. PromptTemplate\n",
    "- **Description:**\n",
    "  - A general-purpose template for creating prompts.\n",
    "  - Supports variables that can be substituted with dynamic values.\n",
    "- **Use Case:**\n",
    "  - Works best for single-shot or basic scenarios, such as question-answering or text generation.\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. ChatPromptTemplate\n",
    "- **Description:**\n",
    "  - A specialized template for managing multi-turn conversations.\n",
    "  - Combines multiple message types (e.g., system, human, AI).\n",
    "- **Use Case:**\n",
    "  - Ideal for chat-based interactions with Large Language Models (LLMs) like ChatGPT.\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. SystemMessagePromptTemplate\n",
    "- **Description:**\n",
    "  - Represents system-level instructions that guide the AI's behavior.\n",
    "  - Defines the role, tone, or rules for the model during the interaction.\n",
    "- **Use Case:**\n",
    "  - Used at the beginning of a chat to set the AI's context.\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. HumanMessagePromptTemplate\n",
    "- **Description:**\n",
    "  - Represents messages from the user in a chat.\n",
    "  - Typically contains user queries or instructions.\n",
    "- **Use Case:**\n",
    "  - Models conversational input from a human.\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. AIMessagePromptTemplate\n",
    "- **Description:**\n",
    "  - Represents messages generated by the AI in a conversation.\n",
    "  - Models AI responses to user inputs.\n",
    "- **Use Case:**\n",
    "  - Helps create structured AI responses during prompt design.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "- **library:** Following library has those prompts\n",
    "     - from langchain_core.prompts import (</br>\n",
    "                                            ChatPromptTemplate,</br>\n",
    "                                            PromptTemplate,</br>\n",
    "                                            SystemMessagePromptTemplate,</br>\n",
    "                                            AIMessagePromptTemplate,</br>\n",
    "                                            HumanMessagePromptTemplate,</br>\n",
    "                                    )\n",
    "    \n",
    "     - **functions:** Following functions commonly used with thoses prompts , How to use will varies prompt to prompt case.\n",
    "        - **from_messages()**   : Works with `ChatPromptTemplate`\n",
    "        - **format_message()**   : Works with `ChatPromptTemplate`\n",
    "        - **format_prompt()**    : Works with all Template Types\n",
    "        - **from_template()**    : Works with all Template Types\n",
    "        - **format()**           : Works with all Template Types\n",
    "        - **invoke()**           : Works with all Template Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following code will fetch the key-value pairs from the environment variables of the program\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # Althouh it will return True if the key-value pairs are fetched successfully, but we don't need to store it in any variable.\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    "cerebras_api_key=os.environ[\"CEREBRAS_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This following syntax is valid for PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# Load environment variables from a .env file.\n",
    "load_dotenv()\n",
    "# For no input, you remove the variables from the template string and make input_variables=[] (i.e., an empty list).\n",
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"level\"],\n",
    "    template=\"Tell me a fact about {topic} for a student {level} level.\"\n",
    ")\n",
    "# Format the prompt, similar to f-string in Python.\n",
    "# prompt=multiple_input_prompt.invoke({\"topic\":'Mars', \"level\":'8th Grade'})\n",
    "# prompt=multiple_input_prompt.format(topic='Mars', level='8th Grade')\n",
    "prompt=multiple_input_prompt.format_prompt(topic='Mars', level='8th Grade')\n",
    "# prompt=multiple_input_prompt.format_message(topic='Mars', level='8th Grade')   # This will not work as no function format_message\n",
    "# Initialize the Google Generative AI model (gemini-1.5-pro).\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "# Invoke the LLM and print the result based on the prompt.\n",
    "response=llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This syntax is valid for ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "]\n",
    "chatprompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "# prompt = chatprompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})                                 \n",
    "# prompt = chatprompt_template.format(topic=\"lawyers\", joke_count=3)\n",
    "# prompt = chatprompt_template.format_prompt(topic=\"lawyers\", joke_count=3)\n",
    "prompt = chatprompt_template.format_prompt(topic=\"lawyers\", joke_count=3).to_messages()\n",
    "# prompt = chatprompt_template.format_messages(topic=\"lawyers\", joke_count=3)\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "print(prompt)\n",
    "# Initialize the Google Generative AI model (gemini-1.5-pro)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "# Invoke the LLM and print the result based on the prompt\n",
    "print(llm.invoke(prompt)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Some Pointers related to message that we supose to pass LLms.\n",
    "##### Note:A scenario that does NOT work\n",
    "\n",
    "- The attempt to use a dynamic human message with placeholders (HumanMessage(content=\"Tell me {joke_count} jokes.\")) does not function as expected.\n",
    "\n",
    "```python\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    HumanMessage(content=\"Tell me {joke_count} jokes.\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Message Passing as a List of Tuples:\n",
    "\n",
    "Here, we need to maintain the order of the messages: **system**, **human**, and **AI**. The **system** is optional. If the **system** is included, it will appear only once and must be placed first, as it is used to set the context of the chat.\n",
    "\n",
    "The pair of **human** and **AI** can appear any number of times. In each pair, **human** must come first, followed by **AI**. At the end, **human** will be the last message.\n",
    "\n",
    "- **Human** \n",
    "- **System** \n",
    "- **AI** \n",
    "\n",
    "##### Some More Example: Using .from_message() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq  # llama-3.1-70b-versatile\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "messages=[\n",
    "    (\"human\", \"What is the capital of {country}?\"),\n",
    "    (\"ai\", \"The capital of {country} is {capital}.\"),\n",
    "    (\"human\", \"Tell me above answer is correct.\"),\n",
    "   \n",
    "]\n",
    "chatprompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "# prompt = chatprompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})                                 \n",
    "# prompt = chatprompt_template.format(topic=\"lawyers\", joke_count=3)\n",
    "# prompt = chatprompt_template.format_prompt(topic=\"lawyers\", joke_count=3)\n",
    "prompt = chatprompt_template.format_prompt(country=\"Canada\",capital=\"Winnipeg\").to_messages()\n",
    "# prompt = prompt_template.format_messages(topic=\"lawyers\", joke_count=3)\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "print(prompt)\n",
    "# Initialize the Google Generative AI model (gemini-1.5-pro)\n",
    "llm = ChatGroq(model=\"llama-3.1-70b-versatile\")\n",
    "# Invoke the LLM and print the result based on the prompt\n",
    "print((llm.invoke(prompt)).content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "from dotenv import load_dotenv # Importing the load_dotenv function from the dotenv module\n",
    "from langchain_core.prompts import (\n",
    "                                            ChatPromptTemplate,\n",
    "                                            PromptTemplate,\n",
    "                                            SystemMessagePromptTemplate,\n",
    "                                            AIMessagePromptTemplate,\n",
    "                                            HumanMessagePromptTemplate,\n",
    "                                    )\n",
    "\n",
    "\n",
    "system_template=\"You are an AI recipe assistant that specializes in {dietary_preference} dishes that can be prepared in {cooking_time}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_template=\"{recipe_request}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "request = chat_prompt.invoke({\"cooking_time\":\"15 min\", \"dietary_preference\":\"Vegan\", \"recipe_request\":\"Quick Snack\"})\n",
    "# request = chat_prompt.format(cooking_time=\"15 min\", dietary_preference=\"Vegan\", recipe_request=\"Quick Snack\")\n",
    "request = chat_prompt.format_messages(cooking_time=\"15 min\", dietary_preference=\"Vegan\", recipe_request=\"Quick Snack\")\n",
    "# request = chat_prompt.format_prompt(cooking_time=\"15 min\", dietary_preference=\"Vegan\", recipe_request=\"Quick Snack\").to_messages()                           \n",
    "load_dotenv() # Loading environment variables from a .env file\n",
    "chat = ChatCohere()\n",
    "result = chat.invoke(request)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This syntax is valid for all types of prompt. using .from_templates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"Tell me a joke about {topic}.\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "print(\"-----Prompt from Template-----\")\n",
    "prompt = prompt_template.invoke({\"topic\": \"dogs\"})\n",
    "\n",
    "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<OR>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "                     \n",
    "prompt = prompt_template.format(topic='dogs')\n",
    "print(prompt)\n",
    "# Initialize the Google Generative AI model (gemini-1.5-pro)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "# Invoke the LLM and print the result based on the prompt\n",
    "print(llm.invoke(prompt)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Shot Types (Categorization Based on Messages or Examples):\n",
    "* These are categories based on the number of examples or context you provide to the model. This refers to how much information (or how many messages/examples) you pass to the model to help it understand the task. For Examples:\n",
    "\n",
    "### Example of Shot Types:\n",
    "\n",
    "- **Zero-shot**: No examples given, the model generates responses based only on instructions.\n",
    "- **One-shot**: A single example is given to help the model understand the task.\n",
    "- **Few-shot**: A few examples are given to the model.\n",
    "- **Many-shot**: Many examples are provided for better context.\n",
    "\n",
    "Shot types categorize prompts based on how much context or how many message examples are included to guide the model in generating a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Prompt Template\n",
    "\n",
    "Let's go over an example where you want a historical conversation to show the LLM Chat Bot a few examples, known as \"Few Shot Prompts\". We essentially build a historical conversation *before* sending the message history to the chat bot. Be careful not too make the entire message too long, as you may hit context limits (but the latest models have quite large contexsts, e.g. GPT-4 has up to 32k tokens at this time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate,SystemMessagePromptTemplate,AIMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "chat = ChatOpenAI(temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating Example Inputs and Outputs\n",
    "template = \"You are a helpful assistant that translates complex legal terms into plain and understandable terms.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "legal_text = \"The provisions herein shall be severable, and if any provision or portion thereof is deemed invalid, illegal, or unenforceable by a court of competent jurisdiction, the remaining provisions or portions thereof shall remain in full force and effect to the maximum extent permitted by law.\"\n",
    "example_input_one = HumanMessagePromptTemplate.from_template(legal_text)\n",
    "\n",
    "plain_text = \"The rules in this agreement can be separated. If a court decides that one rule or part of it is not valid, illegal, or cannot be enforced, the other rules will still apply and be enforced as much as they can under the law.\"\n",
    "example_output_one = AIMessagePromptTemplate.from_template(plain_text)\n",
    "\n",
    "human_template = \"{legal_text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person selling the property, who owns it completely, is transferring all their rights and ownership to the buyer and their successors. This includes any existing debts, claims, or rights that are officially recorded, as well as any rules or limitations that affect the property. This transfer is in exchange for the amount of money paid by the buyer.\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, example_input_one, example_output_one, human_message_prompt]\n",
    ")\n",
    "some_example_text = \"The grantor, being the fee simple owner of the real property herein described, conveys and warrants to the grantee, his heirs and assigns, all of the grantor's right, title, and interest in and to the said property, subject to all existing encumbrances, liens, and easements, as recorded in the official records of the county, and any applicable covenants, conditions, and restrictions affecting the property, in consideration of the sum of [purchase price] paid by the grantee.\"\n",
    "request = chat_prompt.format_prompt(legal_text=some_example_text).to_messages()\n",
    "\n",
    "result = chat.invoke(request)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
