{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "* Input: the prompt we send to the LLM.\n",
    "* Output: the response from the LLM.\n",
    "* We can switch LLMs and use several different LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain divides LLMs in two types\n",
    "1. LLM Model: text-completion model.\n",
    "2. Chat Model: converses with a sequence of messages and can have a particular role defined (system prompt). This type has become the most used in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between Chat Models and Completion Models\n",
    "\n",
    "1. **Chat Models**:\n",
    "   - Designed for structured conversational input.\n",
    "   - Use a message-based format where each message has a role (e.g., `user`, `assistant`, `system`) and content.\n",
    "   - Ideal for multi-turn interactions, chatbots, or scenarios requiring explicit context handling.\n",
    "   - Examples: Models like `ChatOpenAI` support chat-based APIs (e.g., `gpt-3.5-turbo`, `gpt-4`).\n",
    "   - ChatOpenAI(model,temperature,max_tokens,api_key)\n",
    "      - e.g model=\"gpt-3.5-turbo\",temperature=0.7,max_tokens=100,openai_api_key\n",
    "   - ChatGoogleGenerativeAI\n",
    "      - e.g model=\"gemini-1.5-flash\",temperature=0.7,max_tokens=100,google_api_key\n",
    "\n",
    "2. **Completion Models**:\n",
    "   - Designed for free-form text input.\n",
    "   - Input is a single string prompt, without predefined roles or structure.\n",
    "   - Suitable for one-off tasks like text completion, summarization, or code generation.\n",
    "   - Examples: Models like `OpenAI` support text-completion APIs (e.g., `gpt-3.5-turbo`).\n",
    "   - OpenAI(model,temperature,max_tokens,api_key)\n",
    "      - e.g model=\"gpt-3.5-turbo\",temperature=0.7,max_tokens=100,api_key\n",
    "\n",
    "   - GoogleGenerativeAI(model,temperature,max_tokens,api_key)\n",
    "      - e.g model=\"gemini-1.5-flash\",temperature=0.7,max_tokens=100,google_api_key\n",
    "\n",
    "| **Aspect**               | **Chat Models**                             | **Completion Models**                     |\n",
    "|--------------------------|---------------------------------------------|------------------------------------------|\n",
    "| **Input Format**         | List of messages with roles.               | Single string prompt.                    |\n",
    "| **Use Case**             | Conversational AI, multi-turn tasks.        | Text completion, summarization, single-turn tasks. |\n",
    "| **Context Handling**     | Maintains multi-turn context effectively.   | Requires explicit context in the prompt. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of LLMs that can work with LangChain\n",
    "* See the list [here](https://python.langchain.com/v0.1/docs/integrations/llms/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `find_dotenv()`\n",
    "- **What it Does**:  \n",
    "  Searches for a `.env` file in the current directory or parent directories.\n",
    "- **Return Value**:  \n",
    "  - If a `.env` file is found: Returns the full path to the `.env` file.  \n",
    "    Example: `/home/user/project/.env`  \n",
    "  - If no `.env` file is found: Returns an empty string (`\"\"`).\n",
    "\n",
    "---\n",
    "\n",
    "### `load_dotenv()`\n",
    "- **What it Does**:  \n",
    "  Loads the key-value pairs from the `.env` file found by `find_dotenv()` into the environment variables of the program.\n",
    "- **Behavior**:  \n",
    "  - If a `.env` file is found: Updates the environment variables with the key-value pairs.  \n",
    "  - If no `.env` file is found: Does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following code will fetch the key-value pairs from the environment variables of the program\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # Althouh it will return True if the key-value pairs are fetched successfully, but we don't need to store it in any variable.\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    "cerebras_api_key=os.environ[\"CEREBRAS_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Completion Model\n",
    "* These were very popular in the earlier era of LLMs but are not as widely used in recent times.\n",
    "* You can find the LangChain documentation about LLM models [here](https://python.langchain.com/v0.1/docs/modules/model_io/llms/).\n",
    "* Getting the response you can use the method like :\n",
    "    - invoke()\n",
    "    - stream()\n",
    "    - batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris. It is known for its iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and the Arc de Triomphe, as well as its cultural attractions, fashion scene, and cuisine. \n",
      "\n",
      "Would you like to know more about Paris?  I can provide you with additional information and interesting facts about the city if so. \n"
     ]
    }
   ],
   "source": [
    "# OpenAI based Completion model\n",
    "# from langchain_openai import OpenAI\n",
    "# # Initialize the OpenAI model\n",
    "# llm_openai = OpenAI()\n",
    "# # Invoke the model with a prompt\n",
    "# response = llm_openai.invoke(\"Name of all the captains who won the World Cup cricket for India\")\n",
    "# # Print the response\n",
    "# print(response)\n",
    "\n",
    "#####################################################\n",
    "# from langchain_google_genai import GoogleGenerativeAI\n",
    "# llm_google=GoogleGenerativeAI(model=\"gemini-1.5-flash\") # Creating an instance of the ChatOpenAI class with a specific API key\n",
    "# response=llm_google.invoke(\"Name of all the captains who won the World Cup cricket for India?\")  # Invoking the instance with a specific message\n",
    "# print(type(response))  # Printing the type of the response received from the instance\n",
    "# print(response)  # Printing the response received from the instance\n",
    "\n",
    "######################################################\n",
    "# Import the necessary class from langchain_ollama\n",
    "# from langchain_ollama import OllamaLLM   # Initialize the Ollama model\n",
    "# ollama_model = OllamaLLM(model=\"llama3.2:1b\")  # You can specify the model you want to use, like \"llama3.2:1b\"\n",
    "# prompt = \"What is the capital of France?\"\n",
    "# response = ollama_model(prompt) # Get the response from the Ollama model\n",
    "# print(response)  # Printing the response received from the instance\n",
    "\n",
    "#######################################################\n",
    "from langchain_cohere.llms import Cohere\n",
    "Coher_model = Cohere()\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = Coher_model.invoke(prompt) # Get the response from the Cohere model\n",
    "print(response)  # Printing the response received from the instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Kapil Dev (1983)\n",
      "2. Mahendra Singh Dhoni (2011)\n",
      "3. Virat Kohli (2019)"
     ]
    }
   ],
   "source": [
    "## OpenAI based Completion model with Stream function\n",
    "from langchain_openai import OpenAI\n",
    "# Initialize the OpenAI model\n",
    "llm_openai = OpenAI()\n",
    "# Invoke the model with a stream\n",
    "response = llm_openai.stream(\"Name of all the captains who won the World Cup cricket for India?\")\n",
    "# Print the response as streaming\n",
    "for chunk in response:\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\nNarendra Modi', '?\\n\\nThe capital of India is New Delhi. ']\n"
     ]
    }
   ],
   "source": [
    "## OpenAI based Completion model with batch function\n",
    "from langchain_openai import OpenAI\n",
    "# Initialize the OpenAI model\n",
    "llm_openai = OpenAI()\n",
    "# Invoke the model with a batch of prompts\n",
    "response = llm_openai.batch([\"Name of the PM of India\", \"What is the name of the capital of India\"])\n",
    "# Print the response as streaming\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chat Model\n",
    "* The general trend after the launch of ChatGPT-4.\n",
    "    * Frequently referred to as a \"Chatbot.\" \n",
    "    * Enables conversations between humans and AI.\n",
    "    * Can include a system prompt to define the tone or role of the AI.\n",
    "* See LangChain documentation about Chat Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/).\n",
    "* By default we will work with ChatOpenAI. See the LangChain documentation page about it  [here](https://python.langchain.com/v0.1/docs/integrations/chat/openai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sacred lands, a creature roams,\n",
      "The holy cow, with gentle tones.\n",
      "Revered and worshipped, a symbol true,\n",
      "Of life and nourishment, for me and you.\n",
      "\n",
      "With horns that shine, and a coat so bright,\n",
      "She grazes peacefully, in the morning light.\n",
      "Her moos echo soft, through the Indian air,\n",
      "As devotees gather, with love and care.\n",
      "\n",
      "A symbol of kindness, and a gentle soul,\n",
      "The holy cow, makes our hearts whole.\n",
      "Respected and cherished, in every way,\n",
      "A sacred being, in a sacred day."
     ]
    }
   ],
   "source": [
    "## OpenAI based Chat model\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# # Initialize the OpenAI model\n",
    "# llm_openai = ChatOpenAI()\n",
    "# # Invoke the model with a prompt\n",
    "# response = llm_openai.invoke(\"Name of all the captains who won the World Cup cricket for India\")\n",
    "# # Print the response\n",
    "# print(response)\n",
    "# print(response.content) \n",
    "\n",
    "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#\n",
    "## Google Generative AI based Chat model\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# # Initialize the Google Generative AI model\n",
    "# llm_google = ChatGoogleGenerativeAI(  # Creating an instance of the ChatOpenAI class with a specific API key\n",
    "#     model=\"gemini-1.5-flash\"\n",
    "# )\n",
    "# response=llm_google.invoke(\"Hello, how are you?\")  # Invoking the instance with a specific message\n",
    "# print(type(response))  # Printing the type of the response received from the instance\n",
    "# print(response.content)  # Printing the response received from the instance \n",
    "\n",
    "##<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#\n",
    "## Ollama based Chat model\n",
    "# from langchain_ollama import ChatOllama   # Initialize the Ollama model\n",
    "# ollama_model = ChatOllama(model=\"llama3.2:1b\")  # You can specify the model you want to use, like \"llama3.2:1b\"\n",
    "# prompt = \"What is the capital of France?\"\n",
    "# response = ollama_model.invoke(prompt) # Get the response from the Ollama model\n",
    "# print(response)  # Printing the response received from the instance\n",
    "# print(response.content)  # Printing the response received from the instance\n",
    "\n",
    "##<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#\n",
    "## Cohere based Chat model\n",
    "# from langchain_cohere.llms import Cohere\n",
    "# Coher_model = Cohere()\n",
    "# prompt = \"What is the capital of France?\"\n",
    "# response = Coher_model.invoke(prompt) # Get the response from the Cohere model\n",
    "# print(response)  # Printing the response received from the instance\n",
    "\n",
    "##<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#\n",
    "## Groq based Chat model\n",
    "# from langchain_groq import ChatGroq\n",
    "# Groq_model = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "# prompt = \"What is the capital of France?\"\n",
    "# response = Groq_model.invoke(prompt) # Get the response from the Cohere model\n",
    "# print(response)  # Printing the response received from the instance\n",
    "# print(response.content)  # Printing the response received from the instance\n",
    "\n",
    "##<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#\n",
    "# from langchain_cerebras import ChatCerebras\n",
    "# Cerebras_model = ChatCerebras(temperature=0, model=\"llama-3.3-70b\")\n",
    "# prompt = \"What is the capital of France?\"\n",
    "# response = Cerebras_model.invoke(prompt) # Get the response from the Cohere model\n",
    "# print(response)  # Printing the response received from the instance\n",
    "# print(response.content)  # Printing the response received from the instance\n",
    "\n",
    "##<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<USE OF stream() and batch() as well >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>#\n",
    "# # Use of Batch\n",
    "# from langchain_cerebras import ChatCerebras\n",
    "# Cerebras_model = ChatCerebras(temperature=0, model=\"llama-3.3-70b\")\n",
    "# prompt = [\"What is the capital of France?\",\"What is the capital of India?\"]\n",
    "# response = Cerebras_model.batch(prompt) # Get the response from the Cohere model\n",
    "# print(response)  # Printing the response received from the instance\n",
    "# print(response[0].content)  # Printing the response received from the instance\n",
    "# print(response[1].content)  # Printing the response received from the instance\n",
    "\n",
    "# Use of Batch\n",
    "from langchain_cerebras import ChatCerebras\n",
    "Cerebras_model = ChatCerebras(temperature=0, model=\"llama-3.3-70b\")\n",
    "prompt = \"Write a poem of apprx 100 words on holy cow\"\n",
    "response = Cerebras_model.stream(prompt) # Get the response from the Cohere model\n",
    "\n",
    "for chunk in response:\n",
    "  print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
